# -*- coding: utf-8 -*-
"""NEW Daniel Hanasab Research Project .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x4hoH5qFjSMsVey7AvTcOPCpEZCa_WWb
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade nltk bertopic umap-learn hdbscan scikit-learn pandas networkx matplotlib PyMuPDF



import re
import nltk
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter
from itertools import combinations

import fitz  # PyMuPDF for extracting text from PDFs

from bertopic import BERTopic

# Download necessary NLTK components
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Function to extract text from PDFs
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = "\n".join(page.get_text() for page in doc)
    return text

# Paths to the uploaded PDF files (Ensure these files are in the same directory as your Jupyter Notebook)
pdf_files = {
    "Houston": "Houston TX Code of Ordinances.pdf",
    "New York": "NYC Zoning Code.pdf",
    "San Francisco": "San Francisco Zoning.pdf",  # Ensure SF file is uploaded
    "Miami":"Miami Zoning Code.pdf"
}

# Extract and preprocess text
text_data = {}
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')  # Also needed for lemmatization
nltk.download('omw-1.4')  # Optional for wordnet dependencies

import os

# Create NLTK directory if it doesn't exist
nltk_data_path = "/root/nltk_data"
os.makedirs(nltk_data_path, exist_ok=True)

# Unzip the uploaded files into the correct location
!unzip -o punkt.zip -d /root/nltk_data/
!unzip -o stopwords.zip -d /root/nltk_data/

import nltk

# Set the NLTK data path to the extracted files
nltk.data.path.append("/root/nltk_data")

# Verify that the data is now available
nltk.download('punkt', download_dir="/root/nltk_data")
nltk.download('stopwords', download_dir="/root/nltk_data")







import nltk
nltk.download('punkt_tab')


def preprocess_text(text):
    text = re.sub(r'\W+', ' ', text.lower())  # Remove non-alphanumeric characters
    tokens = word_tokenize(text)  # Tokenization
    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words and len(w) > 2]  # Remove stopwords & lemmatize
    return " ".join(tokens)

for city, pdf_path in pdf_files.items():
    raw_text = extract_text_from_pdf(pdf_path)
    processed_text = preprocess_text(raw_text)
    text_data[city] = processed_text

# Convert to DataFrame
df = pd.DataFrame(text_data.items(), columns=['City', 'Processed Text'])
df.head()



def split_text_into_chunks(text, chunk_size=5000):
    words = text.split()
    return [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]

# Extract and preprocess text
text_data = []
document_labels = []  # Store the document-city association

for city, pdf_path in pdf_files.items():
    doc = fitz.open(pdf_path)
    raw_text = "\n".join(page.get_text() for page in doc)
    processed_text = preprocess_text(raw_text)  # Preprocess text

    # Split long document into smaller sections
    text_chunks = split_text_into_chunks(processed_text, chunk_size=5000)

    # Store each chunk as a separate document for BERTopic
    text_data.extend(text_chunks)
    document_labels.extend([city] * len(text_chunks))  # Label each chunk with its city

# Convert to DataFrame
df = pd.DataFrame({"City": document_labels, "Processed Text": text_data})
df.head()



from bertopic import BERTopic
import umap

# Use a custom UMAP model to avoid dimensionality reduction issues
umap_model = umap.UMAP(n_neighbors=10, n_components=5, metric='cosine')

# Fit BERTopic model
topic_model = BERTopic(umap_model=umap_model)
topics, probs = topic_model.fit_transform(df["Processed Text"].tolist())

# Assign topics to the DataFrame
df["Topics"] = topics
df.head()



topic_model.visualize_barchart(top_n_topics=10)

topic_model.visualize_distribution(probs)

topic_model.visualize_barchart(top_n_topics=3)

print("Unique topics found:", len(set(topics)))



topic_model = BERTopic(n_gram_range=(2, 3))  # Uses bigrams and trigrams
topics, probs = topic_model.fit_transform(df["Processed Text"].tolist())

topic_model.visualize_barchart(top_n_topics=10)

topic_model = BERTopic(nr_topics="auto")  # Automatically finds the best topic count
topics, probs = topic_model.fit_transform(df["Processed Text"].tolist())

custom_stopwords = ["shall", "section", "code", "permit", "district", "building"]

def preprocess_text(text):
    text = re.sub(r'\W+', ' ', text.lower())  # Remove non-alphanumeric characters
    tokens = word_tokenize(text)  # Tokenization
    tokens = [w for w in tokens if w not in stop_words and w not in custom_stopwords and len(w) > 2]
    return " ".join(tokens)









restrictive_terms = [
    "inspection", "permit", "approval", "variance", "fee",
    "height", "area", "use", "housing", "development",
    "parking", "zoning", "compliance", "review"
]

topic_info = topic_model.get_topic_info()

restrictive_topics = topic_info[
    topic_info["Name"].str.contains("|".join(restrictive_terms), case=False, na=False)
]
display(restrictive_topics)

restrictive_counts = df.groupby("City")["Topics"].apply(lambda x: x.isin(restrictive_topics["Topic"]).sum())
restrictive_counts = restrictive_counts.reset_index(name="Restrictive Mentions")
display(restrictive_counts)





import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
plt.bar(restrictive_counts["City"], restrictive_counts["Restrictive Mentions"], color=["blue", "red", "green"])
plt.xlabel("City")
plt.ylabel("Restrictive Topic Count")
plt.title("Zoning Stringency by City")
plt.show()

permit_mentions = df["Processed Text"].str.count("permit").groupby(df["City"]).sum()
permit_df = permit_mentions.reset_index(name="Permit Requirement Mentions")

plt.figure(figsize=(8,5))
plt.bar(permit_df["City"], permit_df["Permit Requirement Mentions"], color=["blue", "red", "green"])
plt.xlabel("City")
plt.ylabel("Mentions of 'Permit'")
plt.title("Permit Requirements by City")
plt.show()





# Define common zoning permit-related words to look for
permit_keywords = ["building permit", "variance", "special use", "zoning approval", "conditional use"]

# Count occurrences of each permit type per city
permit_mentions = {}

for city in df["City"].unique():
    city_text = " ".join(df[df["City"] == city]["Processed Text"])
    permit_mentions[city] = {term: city_text.count(term) for term in permit_keywords}

# Convert to DataFrame
permit_df = pd.DataFrame.from_dict(permit_mentions, orient="index")
permit_df = permit_df.reset_index().rename(columns={"index": "City"})

# Display permit counts per city
display(permit_df)

# Plot the most common permits per city
permit_df.set_index("City").plot(kind="bar", figsize=(10,5), colormap="viridis")
plt.xlabel("City")
plt.ylabel("Permit Mentions")
plt.title("Comparison of Permit Requirements by City")
plt.legend(title="Permit Type")
plt.show()



# Example data: Median home prices in 2024 (REPLACE WITH ACTUAL DATA)
real_estate_data = {
    "City": ["Houston", "New York", "San Francisco"],
    "Median Home Price ($)": [350000, 800000, 1300000],
    "Population Density (per sq. mi)": [3800, 28000, 18500],  # Example values
    "Avg Building Permit Approval Time (days)": [14, 90, 120]  # Example values
}

# Convert to DataFrame
real_estate_df = pd.DataFrame(real_estate_data)

# Merge with zoning stringency data
final_comparison = restrictive_counts.merge(real_estate_df, on="City")

# Display comparison table
display(final_comparison)



from textblob import TextBlob

# Calculate sentiment for each city's zoning text
sentiment_scores = {}
for city in df["City"].unique():
    city_text = " ".join(df[df["City"] == city]["Processed Text"])
    sentiment_scores[city] = TextBlob(city_text).sentiment.polarity

# Convert to DataFrame
sentiment_df = pd.DataFrame(list(sentiment_scores.items()), columns=["City", "Sentiment Score"])
print(sentiment_df)

import seaborn as sns
import matplotlib.pyplot as plt

# Define colors: Red for restrictive (negative sentiment), Green for permissive
colors = ["red" if score < 0 else "green" for score in sentiment_df["Sentiment Score"]]

# Create bar plot
plt.figure(figsize=(8,5))
sns.barplot(
    x="City",
    y="Sentiment Score",
    data=sentiment_df,
    palette=colors
)

# Add neutral reference line
plt.axhline(y=0, color="black", linestyle="--", linewidth=1.2, label="Neutral Sentiment")

# Add data labels
for index, row in sentiment_df.iterrows():
    plt.text(index, row["Sentiment Score"] + 0.005, f'{row["Sentiment Score"]:.2f}',
             ha='center', fontsize=10, fontweight='bold', color="black")

# Customize plot
plt.xlabel("City", fontsize=12)
plt.ylabel("Sentiment Score (-1 = Restrictive, +1 = Permissive)", fontsize=12)
plt.title("Zoning Code Sentiment Analysis", fontsize=14)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.legend()
plt.grid(True)

plt.show()









import networkx as nx
from collections import Counter
from itertools import combinations

def build_co_occurrence_graph(text, top_n=50):
    words = text.split()
    word_counts = Counter(words)
    top_words = [word for word, _ in word_counts.most_common(top_n)]

    # Create an edge list
    edges = []
    window_size = 5  # Look at nearby words within a window
    for i in range(len(words) - window_size):
        window_words = words[i:i + window_size]
        for word1, word2 in combinations(window_words, 2):
            if word1 in top_words and word2 in top_words:
                edges.append((word1, word2))

    # Create graph
    G = nx.Graph()
    G.add_edges_from(edges)
    return G

# Analyze zoning complexity via network structure
for city in df["City"].unique():
    print(f"\n🔹 **Building zoning complexity graph for {city}** 🔹")
    city_text = " ".join(df[df["City"] == city]["Processed Text"])
    G = build_co_occurrence_graph(city_text)

    # Compute centrality
    degree_centrality = nx.degree_centrality(G)
    top_nodes = sorted(degree_centrality, key=degree_centrality.get, reverse=True)[:10]

    # Plot network graph
    plt.figure(figsize=(10, 7))
    pos = nx.spring_layout(G, k=0.3)
    nx.draw(G, pos, with_labels=True, node_size=[v * 5000 for v in degree_centrality.values()], font_size=10)
    plt.title(f"Zoning Code Complexity - {city}")
    plt.show()

    print(f"Top 10 influential zoning terms in {city}: {top_nodes}")

import networkx as nx
import matplotlib.pyplot as plt
from collections import Counter

# Function to simplify and visualize the zoning network for each city
def visualize_simplified_network(G, city, top_n=40):
    degree_centrality = nx.degree_centrality(G)

    # Select top influential zoning terms
    important_nodes = sorted(degree_centrality, key=degree_centrality.get, reverse=True)[:top_n]

    # Create a subgraph
    G_filtered = G.subgraph(important_nodes)

    # Adjust node size based on centrality
    node_sizes = [degree_centrality[node] * 5000 for node in G_filtered.nodes()]

    # Generate layout and colors
    plt.figure(figsize=(12, 8))
    pos = nx.spring_layout(G_filtered, k=0.5)
    node_colors = [degree_centrality[node] for node in G_filtered.nodes()]

    nx.draw(G_filtered, pos, with_labels=True, node_size=node_sizes,
            node_color=node_colors, cmap=plt.cm.coolwarm, edge_color="gray", font_size=10)

    plt.title(f"Zoning Code Complexity - {city}", fontsize=14)
    plt.show()

# Generate network graphs for each city
for city in df["City"].unique():
    print(f"\n🔹 **Generating Zoning Complexity Graph for {city}** 🔹")

    # Extract city-specific text
    city_text = " ".join(df[df["City"] == city]["Processed Text"])

    # Create co-occurrence graph (same function used before)
    G = build_co_occurrence_graph(city_text, top_n=50)

    # Visualize the refined network graph
    visualize_simplified_network(G, city)

#🔴 Red Nodes → Most Central Zoning Terms (Highly connected words that appear frequently in zoning regulations).#
#🔵 Blue Nodes → Less Central Terms (Less frequent, but still relevant zoning terms).
#🟠 Orange/Yellow Shades → Medium Importance Terms (Appear frequently but not as interconnected as the red ones).









# Example: Median home prices in 2024 (Replace with actual data)
real_estate_data = {
    "City": ["Houston", "New York", "San Francisco", "Miami"],
    "Median Home Price ($)": [350000, 800000, 1300000, 500000],  # Example numbers
}

# Convert to DataFrame
real_estate_df = pd.DataFrame(real_estate_data)

# Merge with zoning stringency
final_comparison = restrictive_counts.merge(real_estate_df, on="City")

# Scatter plot: Does zoning affect home prices?
plt.figure(figsize=(8,5))
sns.scatterplot(data=final_comparison, x="Restrictive Mentions", y="Median Home Price ($)", hue="City", s=200)
plt.xlabel("Zoning Stringency (Restrictive Mentions)")
plt.ylabel("Median Home Price ($)")
plt.title("Does Zoning Stringency Impact Home Prices?")
plt.grid(True)
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Updated Median Home Prices (Early 2025 Data)
real_estate_data = {
    "City": ["Houston", "New York", "San Francisco", "Miami"],
    "Median Home Price ($)": [325000, 763358, 1390000, 675000],  # Updated values
}

# Convert to DataFrame
real_estate_df = pd.DataFrame(real_estate_data)

# Merge with zoning stringency data
final_comparison = restrictive_counts.merge(real_estate_df, on="City")

# Scatter plot: Does zoning affect home prices?
plt.figure(figsize=(8,5))
sns.scatterplot(
    data=final_comparison,
    x="Restrictive Mentions",
    y="Median Home Price ($)",
    hue="City",
    s=200
)
plt.xlabel("Zoning Stringency (Restrictive Mentions)", fontsize=12)
plt.ylabel("Median Home Price ($)", fontsize=12)
plt.title("Does Zoning Stringency Impact Home Prices?", fontsize=14)
plt.grid(True)
plt.show()




'''
1. Average Home Prices:

    New York City:
        According to Zillow, the average home value in New York, NY, is $763,358, reflecting a 4.2% increase over the past year. ​
        Zillow

    San Francisco:
        Recent reports indicate that San Francisco has experienced a notable decline in housing prices, returning to pre-pandemic levels, primarily due to widespread layoffs in the tech sector. ​
        New York Post

    Houston:
        The median sale price of a home in Houston was $335,000 last month, up 5.7% since last year. ​
        Redfin

'''



import seaborn as sns
import numpy as np

# Compute correlation matrix
correlation_matrix = final_comparison.drop(columns=["City"]).corr()

# Plot heatmap
plt.figure(figsize=(6, 5))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", linewidths=0.5, fmt=".2f")

plt.title("Correlation Between Zoning Stringency & Home Prices")
plt.show()



import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Updated dataset with both home prices and rental prices
real_estate_data = {
    "City": ["Houston", "New York", "San Francisco", "Miami"],
    "Median Home Price ($)": [325000, 763358, 1390000, 675000],  # 2025 home prices
    "Average Rent ($)": [1287, 4370, 3020, 2328]  # 2025 rent values
}

# Convert to DataFrame
real_estate_df = pd.DataFrame(real_estate_data)

# Merge with zoning stringency data
final_comparison = restrictive_counts.merge(real_estate_df, on="City")

# Scatter plot: Home Prices vs. Rent
plt.figure(figsize=(8, 5))
sns.scatterplot(
    x="Median Home Price ($)",
    y="Average Rent ($)",
    hue="City",
    data=final_comparison,
    s=200
)

plt.xlabel("Median Home Price ($)", fontsize=12)
plt.ylabel("Average Rent ($)", fontsize=12)
plt.title("Relationship Between Home Prices and Rent", fontsize=14)
plt.grid(True)
plt.show()

plt.figure(figsize=(8, 5))

# Bar plot comparing zoning stringency with rental prices
sns.barplot(x="City", y="Average Rent ($)", data=final_comparison, palette="coolwarm")

plt.xlabel("City", fontsize=12)
plt.ylabel("Average Rent ($)", fontsize=12)
plt.title("Impact of Zoning Stringency on Rental Prices", fontsize=14)
plt.grid(True)

plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Create figure and axis
fig, ax1 = plt.subplots(figsize=(8,5))

# Bar chart for rental prices
sns.barplot(x="City", y="Average Rent ($)", data=final_comparison, palette="coolwarm", ax=ax1)
ax1.set_xlabel("City", fontsize=12)
ax1.set_ylabel("Average Rent ($)", fontsize=12, color="blue")
ax1.tick_params(axis="y", labelcolor="blue")

# Second y-axis for zoning stringency
ax2 = ax1.twinx()
sns.lineplot(x="City", y="Restrictive Mentions", data=final_comparison, marker="o", color="black", linewidth=2, ax=ax2)
ax2.set_ylabel("Zoning Stringency (Restrictive Mentions)", fontsize=12, color="black")
ax2.tick_params(axis="y", labelcolor="black")

# Title
plt.title("Zoning Stringency vs. Rental Prices", fontsize=14)
plt.grid(True)

plt.show()











!pip install textstat

import textstat
print("✅ textstat is installed and working!")

from textstat import flesch_reading_ease

# Compute readability and complexity per city
complexity_scores = {}
for city in df["City"].unique():
    city_text = " ".join(df[df["City"] == city]["Processed Text"])
    total_words = len(city_text.split())
    avg_sentence_length = total_words / max(city_text.count("."), 1)  # Avoid division by zero
    readability = flesch_reading_ease(city_text)

    complexity_scores[city] = {
        "Total Words": total_words,
        "Avg Sentence Length": avg_sentence_length,
        "Readability Score": readability
    }

# Convert to DataFrame
complexity_df = pd.DataFrame.from_dict(complexity_scores, orient="index").reset_index().rename(columns={"index": "City"})

# Plot readability scores
plt.figure(figsize=(8,5))
plt.bar(complexity_df["City"], complexity_df["Readability Score"], color=["blue", "red", "green"])
plt.xlabel("City")
plt.ylabel("Readability Score")
plt.title("Zoning Code Readability Comparison")
plt.show()

# Convert negative scores to absolute values for better scaling
complexity_df["Readability Score"] = complexity_df["Readability Score"].abs()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
sns.barplot(
    x="City",
    y="Readability Score",
    data=complexity_df,
    palette=["blue", "red", "green", "purple"]
)

plt.xlabel("City", fontsize=12)
plt.ylabel("Flesch Readability Score (Higher = Easier to Read)", fontsize=12)
plt.title("Comparing Zoning Code Readability by City", fontsize=14)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.axhline(y=60, color="gray", linestyle="--", label="Standard Reading Level (60)")
plt.legend()

plt.show()









# Count total number of chunks per city
doc_counts = df["City"].value_counts().sort_index()

# Normalize Restrictive Mentions by chunk count
normalized_restrictive = restrictive_counts.copy()
normalized_restrictive["Normalized Restrictive Mentions"] = normalized_restrictive["Restrictive Mentions"] / doc_counts.values

# Round and display
print("🔹 Normalized Restrictive Mentions:")
display(normalized_restrictive[["City", "Restrictive Mentions", "Normalized Restrictive Mentions"]].round(3))


#Interpretation
#Houston	Only 27.8% of its zoning chunks contained restrictive language → Least restrictive
#Miami	~97.4% of chunks were restrictive → Surprisingly stringent zoning tone
#New York	100% of chunks flagged as restrictive → Very restrictive zoning framework
#San Francisco	100% restrictive mentions → Very high regulatory density

# Match counts by city name using mapping
doc_counts = df["City"].value_counts()

# Merge with restrictive counts
normalized_restrictive = restrictive_counts.copy()
normalized_restrictive["Chunks"] = normalized_restrictive["City"].map(doc_counts)
normalized_restrictive["Normalized Restrictive Mentions"] = (
    normalized_restrictive["Restrictive Mentions"] / normalized_restrictive["Chunks"]
)

# Display rounded output
display(normalized_restrictive[["City", "Restrictive Mentions", "Normalized Restrictive Mentions"]].round(3))



print(df["City"].value_counts())

print(restrictive_counts)



import matplotlib.pyplot as plt

normalized_restrictive.sort_values("Normalized Restrictive Mentions", ascending=False, inplace=True)
ax = normalized_restrictive.plot(kind="bar", x="City", y=["Restrictive Mentions", "Normalized Restrictive Mentions"], figsize=(10, 6))
plt.title("Restrictive Mentions (Raw vs Normalized)")
plt.ylabel("Counts / Normalized Score")
plt.grid(True)
plt.show()



import matplotlib.pyplot as plt

# Sort data for better visual
restrictive_counts_sorted = restrictive_counts.sort_values("Restrictive Mentions", ascending=False)

# Plot
plt.figure(figsize=(8, 6))
plt.bar(restrictive_counts_sorted["City"], restrictive_counts_sorted["Restrictive Mentions"])
plt.title("Restrictive Mentions by City")
plt.xlabel("City")
plt.ylabel("Restrictive Mentions")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()







print("🔹 Sentiment Scores:")
display(sentiment_df.round(3))
#Houston	0.030	Slightly positive tone; relatively neutral, simple language.
#Miami	0.036	Also slightly positive; neutral, not overly restrictive.
#San Francisco	0.048	More positive than Houston/Miami, but likely due to formal/legal phrasing rather than tone.
#New York	0.056	Surprisingly the most “positive,” possibly due to technical clarity or legal phrasing.





print(type(text_data))
print(text_data[:2])  # preview first couple elements





import seaborn as sns
import matplotlib.pyplot as plt

city_counts = df['City'].value_counts()

plt.figure(figsize=(8, 5))
sns.barplot(x=city_counts.index, y=city_counts.values)
plt.title("Number of Text Chunks per City")
plt.ylabel("Number of Chunks")
plt.xlabel("City")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

df['Num_Words'] = df['Processed Text'].apply(lambda x: len(x.split()))
avg_words = df.groupby('City')['Num_Words'].mean()

avg_words.plot(kind='bar', figsize=(8, 5), title='Average Words per Chunk')
plt.ylabel("Avg Words per Chunk")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

from collections import Counter

all_words = " ".join(df['Processed Text']).split()
common_words = Counter(all_words).most_common(10)
print("Top 10 Words Across All Cities:")
for word, freq in common_words:
    print(f"{word}: {freq}")

from wordcloud import WordCloud

for city in df['City'].unique():
    city_text = " ".join(df[df['City'] == city]['Processed Text'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(city_text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Word Cloud - {city}")
    plt.show()



from nltk.tokenize import sent_tokenize
from collections import Counter

# Aggregate all processed text chunks per city into one document
city_docs = df.groupby('City')['Processed Text'].apply(lambda x: ' '.join(x)).to_dict()

# Collect statistics
for city, text in city_docs.items():
    tokens = word_tokenize(text)
    words = [w for w in tokens if w.isalpha()]
    sentences = sent_tokenize(text)
    word_count = len(words)
    sentence_count = len(sentences)
    common_words = Counter(words).most_common(10)

    print(f"\n📘 {city}")
    print(f"• Total Words: {word_count}")
    print(f"• Total Sentences: {sentence_count}")
    print("• Top 10 Words:")
    for word, count in common_words:
        print(f"  - {word}: {count}")





from nltk.tokenize import sent_tokenize
from collections import Counter

# Aggregate all processed text chunks per city into one document
city_docs = df.groupby('City')['Processed Text'].apply(lambda x: ' '.join(x)).to_dict()

# Collect statistics
for city, text in city_docs.items():
    tokens = word_tokenize(text)
    words = [w for w in tokens if w.isalpha()]
    sentences = sent_tokenize(text)
    word_count = len(words)
    sentence_count = len(sentences)
    common_words = Counter(words).most_common(10)

    print(f"\n📘 {city}")
    print(f"• Total Words: {word_count}")
    print(f"• Total Sentences: {sentence_count}")
    print("• Top 10 Words:")
    for word, count in common_words:
        print(f"  - {word}: {count}")





